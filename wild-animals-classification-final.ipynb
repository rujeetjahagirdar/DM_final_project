{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n!pip install --upgrade tensorflow==2.8.0\n!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -y\nimport tensorflow as tf\n\nfrom tensorflow.keras import datasets, layers, models\n\nprint(tf.__version__)\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets,transforms,io\nfrom torchvision.transforms import ToTensor\nfrom torch import utils\nfrom collections import Counter\nfrom PIL import Image\nimport numpy as np\nimage_root_folder='/kaggle/input/wildanimal512validfiles2/resize_512_2'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"def verify_image(i):\n    try:\n        img=Image.open(i)\n        img.verify()\n        return True\n    except Exception:\n            return False\ntransform = transforms.Compose([transforms.Resize((512,512)),transforms.ToTensor()])\ndataset = datasets.ImageFolder(image_root_folder,transform=transform,is_valid_file=verify_image)\nlen(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size=int(len(dataset)*0.6)\nvalidation_size=int((len(dataset)-train_size)*0.2)\ntest_size=int(len(dataset)-(train_size+validation_size))\n# print(train_size,validation_size,test_size)\ntrain,validation,test = utils.data.random_split(dataset,[train_size,validation_size,test_size])\n\ntrainDataLoader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\nvalidationDataLoder = torch.utils.data.DataLoader(validation, batch_size=32, shuffle=True)\ntestDataLoader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Implementation from Scratch","metadata":{}},{"cell_type":"code","source":"class CNN4(torch.nn.Module):\n    __name__=\"Model4\"\n    def __init__(self):\n        super(CNN4, self).__init__()\n        ###############################\n        # Original Input image: (512,512,3)\n        # Conv : (512,512,16)\n        # Pool: (256,256,16)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ########################################\n        # Input Image: (256,256,16)\n        # Conv: (256,256,64)\n        # Pool: (128,128,64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ###############################\n        # Input image: (128,128,64)\n        # Conv : (128,128,128)\n        # Pool: (64,64,128)\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ###############################\n        # Input image: (64,64,128)\n        # Conv : (64,64,512)\n        # Pool: (32,32,512)\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ############################################\n        # FC 28*28*128 -> 625\n        self.fc1 = torch.nn.Linear(32*32 * 512, 256, bias=True) # size of image input to this layer * 128\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        self.layer5 = torch.nn.Sequential(\n            self.fc1,\n            torch.nn.ReLU()\n        )\n        ############################################\n        # FC 28*28*128 -> 625\n        self.fc2 = torch.nn.Linear(256,512, bias=True) # size of image input to this layer * 128\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        self.layer6 = torch.nn.Sequential(\n            self.fc2,\n            torch.nn.ReLU()\n        )\n        ############################################\n        # FC 256 -> 3 Classes\n        self.fc3 = torch.nn.Linear(512, 6, bias=True) # size of image input to this layer * 128\n        torch.nn.init.xavier_uniform_(self.fc3.weight)\n        self.layer7 = torch.nn.Sequential(\n            self.fc3,\n            torch.nn.ReLU()\n        )\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n#         print('After layer4: ',out.shape)\n        out = out.view(-1,512*32*32)   # Flatten them for FC\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n#instantiate CNN model\nmodel4 = CNN4()\nmodel4\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel4.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN3(torch.nn.Module):\n    __name__=\"Model3\"\n    def __init__(self):\n        super(CNN3, self).__init__()\n        ###############################\n        # Original Input image: (512,512,3)\n        # Conv : (512,512,16)\n        # Pool: (256,256,16)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 16, kernel_size=3),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ########################################\n        # Input Image: (256,256,16)\n        # Conv: (256,256,64)\n        # Pool: (128,128,64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(16, 64, kernel_size=3),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ###############################\n        # Input image: (128,128,64)\n        # Conv : (128,128,128)\n        # Pool: (64,64,128)\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        ############################################\n        # FC 28*28*128 -> 625\n        self.fc1 = torch.nn.Linear(62*62*128, 128, bias=True) # size of image input to this layer * 128\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        self.layer4 = torch.nn.Sequential(\n            self.fc1,\n            torch.nn.ReLU()\n        )\n        ############################################\n        # FC 256 -> 3 Classes\n        self.fc2 = torch.nn.Linear(128, 6, bias=True) # size of image input to this layer * 128\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        self.layer5 = torch.nn.Sequential(\n            self.fc2,\n            torch.nn.ReLU()\n        )\n\n    def forward(self, x):\n#         print('Shape of input:',x.shape)\n        out = self.layer1(x)\n#         print('Ouput after layer1:',out.shape)\n#         print(out.dim())\n        out = self.layer2(out)\n#         print('Ouput after layer2:',out.shape)\n        out = self.layer3(out)\n#         print('Ouput after layer3:',out.shape)\n#         out = out.view(out.size(0),-1)   # Flatten them for FC\n        out = out.view(-1,62*62*128)\n#         print('Ouput after flatning:',out.shape)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n\n#instantiate CNN model\nmodel3 = CNN3()\nmodel3\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel3.to(device)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_cnn_function(no_epochos,lr,model):\n    import torch.optim as optim\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\n    no_of_epochos = no_epochos\n    for epoch in range(no_of_epochos):\n        running_loss = 0.0\n#         for i,data in enumerate(trainDataLoader):\n        for batch in trainDataLoader:\n            img,lable=batch[0],batch[1]\n            inputData , lable = img.to(device), lable.to(device)\n            optimizer.zero_grad()\n            output = model(inputData)\n            loss = criterion(output,lable)\n            loss.backward()\n            optimizer.step()\n            running_loss = running_loss+loss.item()\n            if i % 5 == 4:    # print every 100 mini-batches\n                print('Epoch={} Batch={} Loss= {}'.format(epoch + 1, i + 1, running_loss / 5))\n                running_loss = 0.0\n        print(\"####Finished Training######\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_validation_function(model):\n    valcorrect = 0\n    valtotal = 0\n    with torch.no_grad():\n        for data in validationDataLoder:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            valtotal = valtotal + labels.size(0)\n            valcorrect = valcorrect + (predicted == labels).sum().item()\n\n    print('Accuracy of the network on validation images: %d %%' % (\n        100 * valcorrect / valtotal))\n    return(valcorrect / valtotal)\ndef test_test_function(model):\n    testcorrect = 0\n    testtotal = 0\n    with torch.no_grad():\n        for data in testDataLoader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            testtotal = testtotal + labels.size(0)\n            testcorrect = testcorrect + (predicted == labels).sum().item()\n\n    print('Accuracy of the network on test images: %d %%' % (\n        100 * testcorrect / testtotal))\n    return(testcorrect / testtotal)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom datetime import datetime\n# test_accuracy=[]\nno_epo_list =[1,3,5,7]\n# no_epo_list =[3]\nlr_list=[0.001,0.01]\n# lr_list=[0.01]\noverallData=[]\ncombined = list(itertools.product(no_epo_list,lr_list))\n# models={'2':model2,'3':model3,'4':model4}\nmodels={'4':model4}\nfor model in models:\n    validation_accuracy=[]\n    runningTime=[]\n    print(\"$$$$$$$$$$$$$$$$$$$$$$$$ For model with {} layers $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\".format(model))\n    for i in range(len(combined)):\n        print(\"Started training for parametrs: {}\".format(combined[i]))\n        start = datetime.now()\n        train_cnn_function(combined[i][0],combined[i][1],models[model])\n        end = datetime.now()\n        print(end-start)\n        totalTime=round((end - start).seconds+((end-start).microseconds /1000000),4)\n        runningTime.append(totalTime)\n        result = test_validation_function(models[model])\n        validation_accuracy.append(result)\n    #     test_accuracy.append(result[1])\n        print(\"Params: {},validation_accu:{}, time={} secs\".format(combined[i],result,totalTime))\n        print(\"______________________________________________________\")\n    # print(combined)\n    # print(validation_accuracy)\n    # print(test_accuracy)\n              ####Table print#####\n    from tabulate import tabulate\n    data=[]\n    for i in range(len(combined)):\n        data.append([str(model)+\" layers\",combined[i],validation_accuracy[i],runningTime[i]])\n    print(\"\\n\")\n    overallData.extend(data)\n    print(tabulate(data,headers=['Model Layers','Parameters(Epochs,LR)','Validation Accu','Running Time(seconds)']))\n    bestIndex=-1\n    minTime=float(\"inf\")\n    maxAccu=max(validation_accuracy)\n    for i,v in enumerate(validation_accuracy):\n        if(v==maxAccu):\n            if(runningTime[i]<minTime):\n                minTime=runningTime[i]\n                bestIndex=i\n    print(\"\\nBest Parameters: {}, Validation Acuuracy: {}, Running Time: {}\".format(combined[bestIndex],validation_accuracy[bestIndex],runningTime[bestIndex]))\n    print(\"\\nBased on these parameters, the test accuracy is:\\n\")\n    train_cnn_function(combined[bestIndex][0],combined[bestIndex][1],models[model])\n    print(test_test_function(models[model]))\n    print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\")\nprint(\"Overall Performance\\n\")\nprint(tabulate(overallData,headers=['Model Layers','Parameters(Epochs,LR)','Validation Accu','Running Time(seconds)']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base 1: ResNet","metadata":{}},{"cell_type":"code","source":"train_dataset= tf.keras.utils.image_dataset_from_directory(\n    image_root_folder,\n    labels='inferred',\n    subset='training',\n    validation_split=0.2,\n    batch_size=32,\n    image_size=(512, 512),\n    shuffle=True,\n    seed=123,\n)\nvalidation_dataset= tf.keras.utils.image_dataset_from_directory(\n    image_root_folder,\n    labels='inferred',\n    validation_split=0.2,\n    subset='validation',\n    batch_size=32,\n    image_size=(512, 512),\n    shuffle=True,\n    seed=123,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nbase1_resnetmodel = models.Sequential()\nbase1_resnetmodel.add(ResNet50(input_shape=(512,512,3),include_top=False, weights='imagenet', pooling='max'))\nfor layer in base1_resnetmodel.layers:\n    layer.trainable = False\nbase1_resnetmodel.add(layers.Dense(64, activation='relu'))\nbase1_resnetmodel.add(layers.Dense(6))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base1_resnetmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base1_resnetmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nbase1_resnetmodel_history = base1_resnetmodel.fit(train_dataset, validation_data = validation_dataset,epochs = 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1, 2, 1)\nplt.plot(range(10),base1_resnetmodel_history.history['accuracy'], label='accuracy')\nplt.plot(range(10),base1_resnetmodel_history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n# plt.ylim([0.5, 1])\nplt.legend(loc='upper right')\nplt.subplot(1, 2, 2)\nplt.plot(range(10),base1_resnetmodel_history.history['loss'], label='loss')\nplt.plot(range(10),base1_resnetmodel_history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n# plt.ylim([0.5, 1])\nplt.legend(loc='upper right')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base 2: kNN","metadata":{}},{"cell_type":"code","source":"train_ds_for_knn=np.zeros(shape=(len(trainDataLoader.dataset),512*512*3))\ntrain_ds_for_knn_lables=np.zeros(shape=(len(trainDataLoader.dataset),1))\nfor i in range(len(trainDataLoader.dataset)):\n    train_ds_for_knn[i]=trainDataLoader.dataset[i][0].view(1,-1).numpy()\n    train_ds_for_knn_lables[i]=trainDataLoader.dataset[i][1]\n\nvalidation_ds_for_knn=np.zeros(shape=(len(validationDataLoder.dataset),512*512*3))\nvalidation_ds_for_knn_lables=np.zeros(shape=(len(validationDataLoder.dataset),1))\nfor i in range(len(validationDataLoder.dataset)):\n    validation_ds_for_knn[i]=validationDataLoder.dataset[i][0].view(1,-1).numpy()\n    validation_ds_for_knn_lables[i]=validationDataLoder.dataset[i][1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nmodel = KNeighborsClassifier(n_neighbors=10,p=2)\nmodel.fit(train_ds_for_knn, train_ds_for_knn_lables.ravel())\nprint(classification_report(validation_ds_for_knn_lables, model.predict(validation_ds_for_knn),target_names=train.dataset.classes))\n#fast-training","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base 3: SVM","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.metrics import classification_report\nbase3_svc_model=svm.SVC(kernel='linear')\nbase3_svc_model.fit(train_ds_for_knn, train_ds_for_knn_lables.ravel())\n# svc.fit(train_ds_for_knn, train_ds_for_knn_lables.ravel())\nprint(classification_report(validation_ds_for_knn_lables, base3_svc_model.predict(validation_ds_for_knn),target_names=train.dataset.classes))\n#slow training","metadata":{},"execution_count":null,"outputs":[]}]}